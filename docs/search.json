[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gabriel Moreira",
    "section": "",
    "text": "Olá,  seja bem vindo (a) ao meu portfólio.\n\n\nMeu nome é Gabriel Moreira!\n\nEu sou um estudante de estatística, me especializando em Data Science.\n\n\nAqui você encontrará informações sobre mim e alguns dos meus projetos."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Principal.html",
    "href": "Principal.html",
    "title": "Gabriel Moreira",
    "section": "",
    "text": "Aqui falarei um pouco sobre mim e sobre minhas experiências acadêmicas e profissionais.\nMeu nome é Gabriel Moreira, estudante de estatística e atuante como cientista de dados. Desde a infância, sempre tive uma grande afinidade com números e operações matemáticas. Em 2019, meu irmão despertou em mim um grande interesse em programação, o que me levou a unir o útil ao agradável, cursando estatística com o objetivo de me tornar um cientista de dados.\nAntes mesmo de ingressar na faculdade, fiz uma pesquisa e identifiquei que a linguagem Python era amplamente utilizada na comunidade de cientistas de dados. Inicialmente, busquei cursos introdutórios online para aprimorar minhas habilidades de programação. Atualmente, embora tenha aprimorado consideravelmente meu conhecimento em Python, confesso que, devido às demandas do curso, tenho uma utilização mais frequente da linguagem R. Além dessas linguagens, também adquiri conhecimentos básicos em SQL para estruturação de bases de dados e execução de consultas.\nNa faculdade, discutimos situações comuns no mercado de trabalho, o que oferece algum preparo aos estudantes. Entretanto, ao progredir no estágio, percebi que muitas vezes as pessoas não têm clareza sobre os problemas ou sequer reconhecem que estão enfrentando um desafio que pode ser tratado. Assim, uma habilidade que desenvolvi consideravelmente nessa experiência foi o que costumam chamar de “entendimento de negócios”. Outro aprendizado significativo foi entender que nem sempre é necessário empregar soluções complexas para problemas simples. Por vezes, uma análise de dados feita com ferramentas acessíveis, como Excel e outras plataformas lowcode, pode fornecer resultados comparáveis a abordagens mais avançadas, como Python. Isso me fez valorizar essas análises mais práticas e eficientes, reconhecendo a importância de escolher a abordagem adequada para cada contexto.\nAo longo desta jornada, adquiri diversas habilidades, incluindo manipulação, processamento e visualização de dados, desenvolvimento, validação e interpretação de modelos de machine learning, automações de rotinas, entre outros. Sinta-se à vontade para explorar meus projetos na seção Projetos!"
  },
  {
    "objectID": "Projetos.html",
    "href": "Projetos.html",
    "title": "Projetos",
    "section": "",
    "text": "Aqui estão alguns projetos nos quais já trabalhei e gostaria de destacar.\n\nPara acessá-los, basta clicar sobre cada um.\n\n\n\n\n\n  \n\n\n\n\nCepScraping\n\n\n\n\n\n\n\nPython\n\n\nWebScraping\n\n\n\n\nExtração de todos os ceps do Brasil utilizando python e o pacote BeautifulSoup4.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorlds Analisis\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nDashBoard\n\n\n\n\nUtilizando dados relacionados aos campeonatos mundiais de League of Legends (LOL) para criar Dashboards interativos no ambiente Shiny.\n\n\n\n\n \n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "Projetos.html#projeto-1",
    "href": "Projetos.html#projeto-1",
    "title": "Projetos de DataScience",
    "section": "Projeto 1",
    "text": "Projeto 1\nProjeto de extração de ceps de todo o brasil com webscrapping"
  },
  {
    "objectID": "Projetos.html#projeto-2",
    "href": "Projetos.html#projeto-2",
    "title": "Projetos de DataScience",
    "section": "Projeto 2",
    "text": "Projeto 2\nMontagem de dashboards com shiny"
  },
  {
    "objectID": "Projetos.html#projeto-1-1",
    "href": "Projetos.html#projeto-1-1",
    "title": "Projetos de DataScience",
    "section": "Projeto 1",
    "text": "Projeto 1\nModelo de recomendação de filmes com KNN"
  },
  {
    "objectID": "Projetos.html#projeto-2-1",
    "href": "Projetos.html#projeto-2-1",
    "title": "Projetos de DataScience",
    "section": "Projeto 2",
    "text": "Projeto 2\nWebscrapping cnn e top250 filmes"
  },
  {
    "objectID": "Projetos.html#projeto-1-2",
    "href": "Projetos.html#projeto-1-2",
    "title": "Projetos de DataScience",
    "section": "Projeto 1",
    "text": "Projeto 1\nGeolocalização em distâncias e pa"
  },
  {
    "objectID": "Projetos.html#projeto-2-2",
    "href": "Projetos.html#projeto-2-2",
    "title": "Projetos de DataScience",
    "section": "Projeto 2",
    "text": "Projeto 2\nModelo de regressão linear para entendimento de lógica entre distância de capital e destino"
  },
  {
    "objectID": "Principal.html#educação",
    "href": "Principal.html#educação",
    "title": "Gabriel Moreira",
    "section": "Educação",
    "text": "Educação\nAtualmente, estou cursando Estatística, com previsão de conclusão em meados de 2026. Desde o início da minha graduação em maio de 2022, tenho imergido em um mundo fascinante de números e análises.\nFaço parte do Programa de Educação Tutorial (PET), onde além de fazer pesquisas e desenvolver novas áreas do conhecimento tenho a oportunidade de participar e até ministrar cursos e eventos ao lado de uma equipe incrível."
  },
  {
    "objectID": "Estudos.html",
    "href": "Estudos.html",
    "title": "Estudos",
    "section": "",
    "text": "Atualmente, estou cursando Estatística na Universidade Federal de Uberlândia (UFU), experiência que me proporciona um acesso abrangente ao vasto universo analítico e probabilístico. Nesse ambiente acadêmico dinâmico, tenho a oportunidade não apenas de absorver os fundamentos teóricos, mas também de aplicá-los em situações práticas, construindo assim uma base sólida para minha jornada profissional. A faculdade contribui bastante para o meu desenvolvimento, tanto em aprendizado quanto em relações sociais."
  },
  {
    "objectID": "Estudos.html#educação",
    "href": "Estudos.html#educação",
    "title": "Estudos",
    "section": "",
    "text": "Atualmente, estou cursando Estatística na Universidade Federal de Uberlândia (UFU), experiência que me proporciona um acesso abrangente ao vasto universo analítico e probabilístico. Nesse ambiente acadêmico dinâmico, tenho a oportunidade não apenas de absorver os fundamentos teóricos, mas também de aplicá-los em situações práticas, construindo assim uma base sólida para minha jornada profissional.\n\n\n\nFaço parte do Programa de Educação Tutorial (PET), onde além de fazer pesquisas e desenvolver novas áreas do conhecimento tenho a oportunidade de participar e até ministrar cursos e eventos ao lado de uma equipe incrível."
  },
  {
    "objectID": "Principal.html#formação",
    "href": "Principal.html#formação",
    "title": "Gabriel Moreira",
    "section": "Formação",
    "text": "Formação\nUniversidade Federal de Uberlândia\nBacharelado em estatística | 2022 - Atualmente"
  },
  {
    "objectID": "Principal.html#experiência-profissional",
    "href": "Principal.html#experiência-profissional",
    "title": "Gabriel Moreira",
    "section": "Experiência profissional",
    "text": "Experiência profissional\nSupporte Full Commerce - Estágio\nCientista de dados | Abril/2023 - Setembro/2023\n\nSupporte Full Commerce - Efetivo\nCientista de dados jr | Setembro/2023 - Atualmente"
  },
  {
    "objectID": "Estudos.html#faculdade",
    "href": "Estudos.html#faculdade",
    "title": "Estudos",
    "section": "",
    "text": "Atualmente, estou cursando Estatística na Universidade Federal de Uberlândia (UFU), experiência que me proporciona um acesso abrangente ao vasto universo analítico e probabilístico. Nesse ambiente acadêmico dinâmico, tenho a oportunidade não apenas de absorver os fundamentos teóricos, mas também de aplicá-los em situações práticas, construindo assim uma base sólida para minha jornada profissional. A faculdade contribui bastante para o meu desenvolvimento, tanto em aprendizado quanto em relações sociais."
  },
  {
    "objectID": "Estudos.html#cursos-livros-certificações",
    "href": "Estudos.html#cursos-livros-certificações",
    "title": "Estudos",
    "section": "Cursos, livros & Certificações",
    "text": "Cursos, livros & Certificações"
  },
  {
    "objectID": "Estudos.html#projetos-de-extensão-extracurriculares",
    "href": "Estudos.html#projetos-de-extensão-extracurriculares",
    "title": "Estudos",
    "section": "Projetos de extensão extracurriculares",
    "text": "Projetos de extensão extracurriculares"
  },
  {
    "objectID": "Principal.html#olá-e-seja-bem-vindo-ao-meu-portfólio",
    "href": "Principal.html#olá-e-seja-bem-vindo-ao-meu-portfólio",
    "title": "Gabriel Moreira",
    "section": "",
    "text": "Aqui falarei um pouco sobre mim e sobre minhas experiências acadêmicas e profissionais.\nMeu nome é Gabriel Moreira, estudante de estatística e atuante como cientista de dados. Desde a infância, sempre tive uma grande afinidade com números e operações matemáticas. Em 2019, meu irmão despertou em mim um grande interesse em programação, o que me levou a unir o útil ao agradável, cursando estatística com o objetivo de me tornar um cientista de dados.\nAntes mesmo de ingressar na faculdade, fiz uma pesquisa e identifiquei que a linguagem Python era amplamente utilizada na comunidade de cientistas de dados. Inicialmente, busquei cursos introdutórios online para aprimorar minhas habilidades de programação. Atualmente, embora tenha aprimorado consideravelmente meu conhecimento em Python, confesso que, devido às demandas do curso, tenho uma utilização mais frequente da linguagem R. Além dessas linguagens, também adquiri conhecimentos básicos em SQL para estruturação de bases de dados e execução de consultas.\nNa faculdade, discutimos situações comuns no mercado de trabalho, o que oferece algum preparo aos estudantes. Entretanto, ao progredir no estágio, percebi que muitas vezes as pessoas não têm clareza sobre os problemas ou sequer reconhecem que estão enfrentando um desafio que pode ser tratado. Assim, uma habilidade que desenvolvi consideravelmente nessa experiência foi o que costumam chamar de “entendimento de negócios”. Outro aprendizado significativo foi entender que nem sempre é necessário empregar soluções complexas para problemas simples. Por vezes, uma análise de dados feita com ferramentas acessíveis, como Excel e outras plataformas lowcode, pode fornecer resultados comparáveis a abordagens mais avançadas, como Python. Isso me fez valorizar essas análises mais práticas e eficientes, reconhecendo a importância de escolher a abordagem adequada para cada contexto.\nAo longo desta jornada, adquiri diversas habilidades, incluindo manipulação, processamento e visualização de dados, desenvolvimento, validação e interpretação de modelos de machine learning, automações de rotinas, entre outros. Sinta-se à vontade para explorar meus projetos na seção Projetos!"
  },
  {
    "objectID": "Principal.html#sobre-mim",
    "href": "Principal.html#sobre-mim",
    "title": "Gabriel Moreira",
    "section": "Sobre mim",
    "text": "Sobre mim\nMeu nome é Gabriel Moreira, estudante de estatística e atuante como cientista de dados. Desde a infância, sempre tive uma grande afinidade com números e operações matemáticas. Em 2019, meu irmão despertou em mim um grande interesse em programação, o que me levou a unir o útil ao agradável, cursando estatística com o objetivo de me tornar um cientista de dados.\nAntes mesmo de ingressar na faculdade, fiz uma pesquisa e identifiquei que a linguagem Python era amplamente utilizada na comunidade de cientistas de dados. Inicialmente, busquei cursos introdutórios online para aprimorar minhas habilidades de programação. Atualmente, embora tenha aprimorado consideravelmente meu conhecimento em Python, confesso que, devido às demandas do curso, tenho uma utilização mais frequente da linguagem R. Além dessas linguagens, também adquiri conhecimentos básicos em SQL para estruturação de bases de dados e execução de consultas.\nNa faculdade, discutimos situações comuns no mercado de trabalho, o que oferece algum preparo aos estudantes. Entretanto, ao progredir no estágio, percebi que muitas vezes as pessoas não têm clareza sobre os problemas ou sequer reconhecem que estão enfrentando um desafio que pode ser tratado. Assim, uma habilidade que desenvolvi consideravelmente nessa experiência foi o que costumam chamar de “entendimento de negócios”. Outro aprendizado significativo foi entender que nem sempre é necessário empregar soluções complexas para problemas simples. Por vezes, uma análise de dados feita com ferramentas acessíveis, como Excel e outras plataformas lowcode, pode fornecer resultados comparáveis a abordagens mais avançadas, como Python. Isso me fez valorizar essas análises mais práticas e eficientes, reconhecendo a importância de escolher a abordagem adequada para cada contexto.\nAo longo desta jornada, adquiri diversas habilidades, incluindo manipulação, processamento e visualização de dados, desenvolvimento, validação e interpretação de modelos de machine learning, automações de rotinas, entre outros. Sinta-se à vontade para explorar meus projetos na seção Projetos!"
  },
  {
    "objectID": "index.html#sobre-mim",
    "href": "index.html#sobre-mim",
    "title": "Gabriel Moreira",
    "section": "Sobre mim",
    "text": "Sobre mim\nMeu nome é Gabriel Moreira, estudante de estatística e atuante como cientista de dados. Desde a infância, sempre tive uma grande afinidade com números e operações matemáticas. Em 2019, meu irmão despertou em mim um grande interesse em programação, o que me levou a unir o útil ao agradável, cursando estatística com o objetivo de me tornar um cientista de dados.\nAntes mesmo de ingressar na faculdade, fiz uma pesquisa e identifiquei que a linguagem Python era amplamente utilizada na comunidade de cientistas de dados. Inicialmente, busquei cursos introdutórios online para aprimorar minhas habilidades de programação. Atualmente, embora tenha aprimorado consideravelmente meu conhecimento em Python, confesso que, devido às demandas do curso, tenho uma utilização mais frequente da linguagem R. Além dessas linguagens, também adquiri conhecimentos básicos em SQL para estruturação de bases de dados e execução de consultas.\nNa faculdade, discutimos situações comuns no mercado de trabalho, o que oferece algum preparo aos estudantes. Entretanto, ao progredir no estágio, percebi que muitas vezes as pessoas não têm clareza sobre os problemas ou sequer reconhecem que estão enfrentando um desafio que pode ser tratado. Assim, uma habilidade que desenvolvi consideravelmente nessa experiência foi o que costumam chamar de “entendimento de negócios”. Outro aprendizado significativo foi entender que nem sempre é necessário empregar soluções complexas para problemas simples. Por vezes, uma análise de dados feita com ferramentas acessíveis, como Excel e outras plataformas lowcode, pode fornecer resultados comparáveis a abordagens mais avançadas, como Python. Isso me fez valorizar essas análises mais práticas e eficientes, reconhecendo a importância de escolher a abordagem adequada para cada contexto.\nAo longo desta jornada, adquiri diversas habilidades, incluindo manipulação, processamento e visualização de dados, desenvolvimento, validação e interpretação de modelos de machine learning, automações de rotinas, entre outros. Sinta-se à vontade para explorar meus projetos na seção Projetos!"
  },
  {
    "objectID": "index.html#formação",
    "href": "index.html#formação",
    "title": "Gabriel Moreira",
    "section": "Formação",
    "text": "Formação\nUniversidade Federal de Uberlândia\nBacharelado em estatística | 2022 - Atualmente"
  },
  {
    "objectID": "index.html#experiência-profissional",
    "href": "index.html#experiência-profissional",
    "title": "Gabriel Moreira",
    "section": "Experiência profissional",
    "text": "Experiência profissional\nSupporte Full Commerce - Estágio\nCientista de dados | Abril/2023 - Setembro/2023\n\nSupporte Full Commerce - Efetivo\nCientista de dados jr | Setembro/2023 - Atualmente"
  },
  {
    "objectID": "Projetos.html#projetos",
    "href": "Projetos.html#projetos",
    "title": "Projetos de DataScience",
    "section": "Projetos",
    "text": "Projetos\n\n\n\n\n  \n\n\n\n\nCepScraping\n\n\n\n\n\n\n\nPython\n\n\nWebScraping\n\n\n\n\nExtração de todos os ceps do Brasil utilizando python e o pacote BeautifulSoup4\n\n\n\n\n \n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html",
    "href": "projetos/BrCepsWebScraping/index.html",
    "title": "CepScraping",
    "section": "",
    "text": "O projeto consiste em extrair informações sobre CEPs de um site que organiza os dados por divisões, com links para cada CEP disponível.\nA abordagem envolve navegar através das diferentes camadas de informação: primeiro acessando o estado, seguido pela cidade, bairro e finalmente, a rua. O processo requer a coleta e armazenamento de todos esses links para, posteriormente, acessar cada rua e extrair as informações específicas necessárias.\nEm resumo, a tarefa principal envolve a navegação hierárquica pelos links para coletar dados precisos sobre CEPs em diferentes localidades."
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html#oi-bom-dia",
    "href": "projetos/BrCepsWebScraping/index.html#oi-bom-dia",
    "title": "CepScrapping",
    "section": "oi bom dia",
    "text": "oi bom dia\naham"
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html#biliotecas",
    "href": "projetos/BrCepsWebScraping/index.html#biliotecas",
    "title": "CepScraping",
    "section": "Biliotecas",
    "text": "Biliotecas\nBeautiful Soup 4, que é uma biblioteca Python usada para extrair dados de HTML. É comumente usada para fazer web scraping.\nRequests, que é uma biblioteca HTTP para Python. Ela permite que você envie solicitações HTTP, facilitando a interação com APIs da web e a obtenção de conteúdo de páginas da web.\nPandas, que é uma poderosa biblioteca para manipulação e análise de dados.\nconcurrent.futures, essa biblioteca faz parte da biblioteca padrão do Python, será utilizado para execução paralela de tarefas.\nPara instalá-las, basta executar o seguinte código:\n\n!pip install beautifulsoup4\n!pip install requests\n!pip install pandas"
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html#código",
    "href": "projetos/BrCepsWebScraping/index.html#código",
    "title": "CepScraping",
    "section": "Código",
    "text": "Código\n\nImportar bibliotecas\n\nimport pandas as pd # Importar Pandas como \"pd\" é uma convenção comum na comunidade Python\nfrom bs4 import BeautifulSoup\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor \n\n\n\n\nCabeçalho de consultas\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n}\n\nAo incluir esse cabeçalho User-Agent em uma solicitação HTTP, você está informando ao servidor web sobre o tipo de navegador e sistema operacional que está sendo usado para acessar a página.\n\n\n\nFunções\n\ndef extract_links(url):\n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    empty_div = soup.find('div', class_='list__empty')\n    if empty_div:\n        print(f\"Não há informações de links para {url}\")\n        return None\n    else:\n        primeira_ul = soup.find('ul', class_='list__list')\n        if primeira_ul:\n            links = [a['href'] for a in primeira_ul.find_all('a')]\n            return links\n        else:\n            return []\n\nEssa função extrai links de uma página da web. Primeiro, faz uma solicitação HTTP para obter o HTML da URL usando requests com um cabeçalho definido. Em seguida, utiliza o BeautifulSoup para analisar o HTML. A função verifica se há uma div com a classe “list__empty”, indicando uma lista vazia de localidades. Se encontrada, imprime uma mensagem e retorna None. Caso contrário, procura a primeira ul com a classe “list__list” e extrai os links, retornando-os como uma lista. Se a lista não for encontrada, retorna uma lista vazia. Resumidamente, a função evita extrair dados de listas vazias e retorna os links encontrados, se existirem\n\ndef extract_links2(url, max_attempts=5):\n    for attempt in range(max_attempts):\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n            empty_div = soup.find('div', class_='list__empty')\n\n            if empty_div:\n                print(f\"Não há informações de links para {url}\")\n                return []\n            else:\n                primeira_ul = soup.find('ul', class_='list__list')\n\n                if primeira_ul:\n                    links = [a['href'] for a in primeira_ul.find_all('a')]\n                    return links\n                else:\n                    return []\n        except Exception as e:\n            print(f\"Tentativa {attempt + 1} de {max_attempts} - Erro ao processar {url}: {e}\")\n            if attempt &lt; max_attempts - 1:\n                print(\"Tentando novamente...\")\n            else:\n                print(\"Número máximo de tentativas atingido. Desistindo.\")\n                return None\n\nA função extract_links2 é uma versão aprimorada da extract_links, trazendo mais robustez e habilidade para lidar com falhas usando tentativas múltiplas e captura de exceções. Essas melhorias serão particularmente valiosas quando precisarmos otimizar e acelerar o processo de extração de dados.\n\ndef extract_rua_data(rua, max_attempts=5):\n    for attempt in range(max_attempts):\n        try:\n            response = requests.get(rua, headers=headers, timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n            busca = soup.find('div', class_=\"top__more-infos\")\n            if busca:\n                texto = busca.text\n                dados = {}\n                for linha in texto.split(\"\\n\"):\n                    if linha.strip():\n                        chave, valor = linha.strip().split(\":\", 1)\n                        dados[chave] = valor\n                return dados\n            else:\n                return None\n        except Exception as e:\n            print(f\"Tentativa {attempt + 1} de {max_attempts} - Erro ao processar {rua}: {e}\")\n            if attempt &lt; max_attempts - 1:\n                print(\"Tentando novamente...\")\n            else:\n                print(\"Número máximo de tentativas atingido. Desistindo.\")\n                return None\n\nEssa função serve para extrair informações do Logradouro, CEP, Bairro, Cidade e Estado do link final (Rua), lidando com erros de requisição e tentativas múltiplas para garantir uma extração mais robusta.\n\n\n\nAcessando e extraindo os links\n\nurl = \"https://listacep.com/\"\n\nestados_links = extract_links(url)\n\ncidades_links = []\nfor estado_link in estados_links:\n    cidades_links.extend(extract_links(estado_link))\n\ncidades_links = list(set(cidades_links)) \ncidades_links = [cidade for cidade in cidades_links if cidade not in estados_links]\n\nA função extract_links é chamada para extrair os links relacionados aos estados do Brasil. O código percorre os links de estados previamente obtidos e, para cada estado, utiliza novamente a função extract_links para extrair os links relacionados às cidades. A lista cidades_links passa por um processo de limpeza evitando duplicatas indesejadas.\n\nwith ThreadPoolExecutor(max_workers=40) as executor:\n    bairros = list(executor.map(extract_links2, cidades_links))\n\nbairros_links = [bairro for sublist in bairros for bairro in sublist]\nbairros_links = list(set(bairros_links))\n\nwith ThreadPoolExecutor(max_workers=40) as executor:\n    ruas = list(executor.map(extract_links2, bairros_links))\n\nruas_links = [rua for sublist in ruas for rua in sublist]\nruas_links = list(set(ruas_links))\n\nEsse código utiliza ThreadPoolExecutor juntamente com a função extract_links2 para coletar paralelamente links de bairros e ruas de forma eficiente, removendo duplicatas nos resultados finais.\n\n\n\nColetando os CEPs\n\nrua_data_list = []\n\nwith ThreadPoolExecutor(max_workers=40) as executor:\n    rua_data_list = list(executor.map(extract_rua_data, ruas_links))\n\nrua_data_list = [data for data in rua_data_list if data]\n\ndf = pd.DataFrame(columns=[\"Logradouro\", \"CEP\", \"Bairro\", \"Cidade\", \"Estado\"])\ndf = df.append(rua_data_list, ignore_index=True)\n\ngrupos_estados = df.groupby('Estado')\n\nwith pd.ExcelWriter('DadosCeps_por_estado.xlsx') as writer:\n    for estado, dados_estado in grupos_estados:\n        dados_estado.to_excel(writer, sheet_name=estado, index=False)\n\nEsta parte do código coleta dados de diversas ruas também em paralelo, organizando esses dados em um DataFrame Pandas e, em seguida, exporta esses dados para um arquivo Excel com abas separadas para cada estado."
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html#explicação-da-aplicação",
    "href": "projetos/BrCepsWebScraping/index.html#explicação-da-aplicação",
    "title": "CepScraping",
    "section": "",
    "text": "O projeto consiste em extrair informações sobre CEPs de um site que organiza os dados por divisões, com links para cada CEP disponível. A abordagem envolve navegar através das diferentes camadas de informação: primeiro acessando o estado, seguido pela cidade, bairro e finalmente, a rua. O processo requer a coleta e armazenamento de todos esses links para, posteriormente, acessar cada cidade e extrair as informações específicas necessárias. Em resumo, a tarefa principal envolve a navegação hierárquica pelos links para coletar dados precisos sobre CEPs em diferentes localidades."
  },
  {
    "objectID": "projetos/BrCepsWebScraping/index.html#explicação-do-projeto",
    "href": "projetos/BrCepsWebScraping/index.html#explicação-do-projeto",
    "title": "CepScraping",
    "section": "",
    "text": "O projeto consiste em extrair informações sobre CEPs de um site que organiza os dados por divisões, com links para cada CEP disponível.\nA abordagem envolve navegar através das diferentes camadas de informação: primeiro acessando o estado, seguido pela cidade, bairro e finalmente, a rua. O processo requer a coleta e armazenamento de todos esses links para, posteriormente, acessar cada rua e extrair as informações específicas necessárias.\nEm resumo, a tarefa principal envolve a navegação hierárquica pelos links para coletar dados precisos sobre CEPs em diferentes localidades."
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "Sobre mim",
    "section": "",
    "text": "Meu nome é Gabriel Moreira, estudante de estatística e atuante como cientista de dados. Desde a infância, sempre tive uma grande afinidade com números e operações matemáticas. Em 2019, meu irmão despertou em mim um grande interesse em programação, o que me levou a unir o útil ao agradável, cursando estatística com o objetivo de me tornar um cientista de dados.\nAntes mesmo de ingressar na faculdade, fiz uma pesquisa e identifiquei que a linguagem Python era amplamente utilizada na comunidade de cientistas de dados. Inicialmente, busquei cursos introdutórios online para aprimorar minhas habilidades de programação. Atualmente, embora tenha aprimorado consideravelmente meu conhecimento em Python, confesso que, devido às demandas do curso, tenho uma utilização mais frequente da linguagem R. Além dessas linguagens, também adquiri conhecimentos básicos em SQL para estruturação de bases de dados e execução de consultas.\nNa faculdade, discutimos situações comuns no mercado de trabalho, o que oferece algum preparo aos estudantes. Entretanto, ao progredir no estágio, percebi que muitas vezes as pessoas não têm clareza sobre os problemas ou sequer reconhecem que estão enfrentando um desafio que pode ser tratado. Assim, uma habilidade que desenvolvi consideravelmente nessa experiência foi o que costumam chamar de “entendimento de negócios”. Outro aprendizado significativo foi entender que nem sempre é necessário empregar soluções complexas para problemas simples. Por vezes, uma análise de dados feita com ferramentas acessíveis, como Excel e outras plataformas lowcode, pode fornecer resultados comparáveis a abordagens mais avançadas, como Python. Isso me fez valorizar essas análises mais práticas e eficientes, reconhecendo a importância de escolher a abordagem adequada para cada contexto.\nAo longo desta jornada, adquiri diversas habilidades, incluindo manipulação, processamento e visualização de dados, desenvolvimento, validação e interpretação de modelos de machine learning, automações de rotinas, entre outros. Sinta-se à vontade para explorar meus projetos na seção Projetos!"
  },
  {
    "objectID": "About.html#formação",
    "href": "About.html#formação",
    "title": "Sobre mim",
    "section": "Formação",
    "text": "Formação\nUniversidade Federal de Uberlândia\nBacharelado em estatística | 2022 - Atualmente"
  },
  {
    "objectID": "About.html#experiência-profissional",
    "href": "About.html#experiência-profissional",
    "title": "Sobre mim",
    "section": "Experiência profissional",
    "text": "Experiência profissional\nSupporte Full Commerce - Efetivo\nCientista de dados jr | Setembro/2023 - Atualmente\n\nSupporte Full Commerce - Estágio\nCientista de dados | Abril/2023 - Setembro/2023"
  },
  {
    "objectID": "index.html#eu-sou-gabriel-moreira",
    "href": "index.html#eu-sou-gabriel-moreira",
    "title": "Gabriel Moreira",
    "section": "",
    "text": "Aqui você encontrará informações sobre mim e alguns dos meus projetos."
  },
  {
    "objectID": "projetos/LolDashIC/index.html",
    "href": "projetos/LolDashIC/index.html",
    "title": "Worlds Analisis",
    "section": "",
    "text": "O objetivo é utilizar os dados coletados dos campeonatos mundiais de League of Legends (LOL) entre os anos de 2011 e 2022, disponibilizados no Kaggle, como base para análise e desenvolvimento de dashboard."
  },
  {
    "objectID": "projetos/LolDashIC/index.html#bibliotecas",
    "href": "projetos/LolDashIC/index.html#bibliotecas",
    "title": "Worlds Analisis",
    "section": "Bibliotecas",
    "text": "Bibliotecas\nTidyverse, fornece ferramentas para manipulação, visualização e análise de dados de maneira consistente e eficiente.\nPlotly, para criação de gráficos interativos e visualizações de dados.\nShiny, para a criação de aplicativos web interativos, permitindo a construção de interfaces dinâmicas e reativas para análise de dados.\nShinyDashboard, extensão do pacote Shiny que simplifica a criação de dashboards interativos."
  },
  {
    "objectID": "projetos/LolDashIC/index.html#análise-exploratória",
    "href": "projetos/LolDashIC/index.html#análise-exploratória",
    "title": "Worlds Analisis",
    "section": "Análise exploratória",
    "text": "Análise exploratória"
  },
  {
    "objectID": "projetos/LolDashIC/index.html#tratativas",
    "href": "projetos/LolDashIC/index.html#tratativas",
    "title": "Worlds Analisis",
    "section": "Tratativas",
    "text": "Tratativas"
  },
  {
    "objectID": "projetos/LolDashIC/index.html#criação-do-dashboard",
    "href": "projetos/LolDashIC/index.html#criação-do-dashboard",
    "title": "Worlds Analisis",
    "section": "Criação do Dashboard",
    "text": "Criação do Dashboard"
  },
  {
    "objectID": "projetos/LolDashIC/index.html#explicação-do-projeto",
    "href": "projetos/LolDashIC/index.html#explicação-do-projeto",
    "title": "Worlds Analisis",
    "section": "",
    "text": "O objetivo é utilizar os dados coletados dos campeonatos mundiais de League of Legends (LOL) entre os anos de 2011 e 2022, disponibilizados no Kaggle, como base para análise e desenvolvimento de dashboard."
  },
  {
    "objectID": "projetos/LolDashIC/index.html#preparando-os-dados",
    "href": "projetos/LolDashIC/index.html#preparando-os-dados",
    "title": "Worlds Analisis",
    "section": "Preparando os dados",
    "text": "Preparando os dados\nNesta seção, estamos iniciando o processo de preparação dos dados para a análise.\n\nBases de dados\nOs dados fundamentais para esta análise estão organizados em três arquivos CSV:\n\nchampions_stats, que inclui estatísticas relacionadas aos personagens do jogo;\nmatchs_stats, que engloba estatísticas de partidas;\nplayer_stats, que contém informações detalhadas sobre os jogadores.\n\n\nChampions &lt;- read.csv(\"champions_stats.csv\")\nMatchs &lt;- read.csv(\"matchs_stats.csv\")\nPlayer &lt;- read.csv(\"players_stats.csv\")\n\nEste trecho de código realiza a leitura dos arquivos CSV e os salva nas variáveis “Champions”, “Matchs” e “Player”.\n\n\nAnálise exploratória\n\nstr(Champions)\n\nA função str() fornece uma visão geral da estrutura do dataframe “Champions”. Ela exibe informações sobre as variáveis, seus tipos de dados e as primeiras observações dos dados.\n\ntable(Champions$season)\n\nAqui, estou utilizando table() para contar a ocorrência de valores únicos na coluna “season” do dataframe “Champions”. Isso é útil para entender a distribuição dos dados ao longo das temporadas.\n\ntable(Champions$event)\n\nEsta linha utiliza table() para contar as diferentes categorias na coluna “event” do dataframe “Champions”. Isso fornece uma visão sobre os tipos de eventos, distinguindo entre jogos classificatórios (“Play in”) e o torneio principal (“Main”).\n\ntable(Champions$champion)\n\nAqui, estou usando table() para contar as ocorrências de cada campeão na coluna “champion” do dataframe “Champions”. Isso ajuda a verificar se existem diferentes formas de nomeação para os mesmos campeões.\n\nstr(Matchs)\n# patch são atualizações mensais que fazem no jogo.\n# os primeiros elementos da coluna patch, tem valores NA pois antigamente não faziam essas atualizações no jogo.\n\nEsta linha novamente utiliza str() para dar uma visão geral da estrutura do dataframe “Matchs”, incluindo informações sobre variáveis e tipos de dados.\n\ntable(Matchs$blue_team)\n# é importante ter atenção no nome dos times pois alguns trocaram de nome, mas represantam a mesma coisa;\n# ex: SK_Telecom_T1 == T1, DWG_KIA == Dplus_KIA,  Gen_g == Gen_2e-G == KSV eSports e etc...\n\nAqui, estou contando as ocorrências dos times presentes na coluna “blue_team” do dataframe “Matchs”. A observação sobre os nomes dos times é relevante para garantir consistência nos dados.\n\nstr(Player)\n\nMais uma vez, str() é utilizado para fornecer informações sobre a estrutura do dataframe “Player”.\n\ntable(Player$player)\n\nEsta linha conta as ocorrências de cada jogador na coluna “player” do dataframe “Player”. A observação sobre a consistência dos nomes dos jogadores é importante para evitar ambiguidades.\n\nPlayer$teste &lt;- ifelse(Player$wins + Player$loses == Player$games_played,0,1)\n\nAqui, estou criando uma nova variável chamada “teste” que indica se a soma de vitórias e derrotas de um jogador é igual ao número total de jogos jogados. Essa linha serve como uma verificação de consistência nos dados.\n\n\nTratando os dados"
  }
]